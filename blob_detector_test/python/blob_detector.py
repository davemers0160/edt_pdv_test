import platform
from pathlib import Path

from cffi import FFI

# numpy/opencv
import numpy as np
# import cv2 as cv

from object_detection.utils import label_map_util as lm_util

from ._video import _VideoAsyncBoxDetector
from ..base import Property
from ..types.array import StateVector
from ..types.detection import Detection


class BlobDetector(_VideoAsyncBoxDetector):
    """BlobDetector

    A box object detector that generates detections of objects in the form of bounding boxes 
    from image/video frames using a BlobDetector. 
    
    The detections generated by the box detector have the form of bounding boxes that capture 
    the area of the frame where an object is detected. Each bounding box is represented by a 
    vector of the form ``[x, y, w, h]``, where ``x, y`` denote the relative coordinates of the 
    top-left corner, while ``w, h`` denote the relative width and height of the bounding box. 
    
    Additionally, each detection carries the following meta-data fields:

    - ``raw_box``: The raw bounding box, as generated by BlobDetector.
    - ``class``: A dict with keys ``id`` and ``name`` relating to the id and name of the 
      detection class.
    - ``score``: A float in the range ``(0, 1]`` indicating the detector's confidence.
    
    Important
    ---------
    Use of this class requires that TensorFlow 2 and the TensorFlow Object Detection API are 
    installed. A quick guide on how to set these up can be found 
    `here <https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/install.html>`_. 
    
    """  # noqa

    model_path: Path = Property(
        doc="Path to ``saved_model`` directory. This is the directory that contains the "
            "``saved_model.pb`` file.")

    labels_path: Path = Property(
        doc="Path to label map (``*.pbtxt`` file). This is the file that contains mapping of "
            "object/class ids to meaningful names")

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

        # modify these to point to the right locations
        if platform.system() == "Windows":
            libname = "blob_detector.dll"
            lib_location = "../../../blob_detector/build/Release/" + libname
        elif platform.system() == "Linux":
            libname = "libblob_detector.so"
            home = os.path.expanduser('~')
            lib_location = home + "../../../blob_detector/build/" + libname
        else:
            quit()
        
        # -----------------------------------------------------------------------------
        # This section allows cffi to get the info about the library and the specialized data types
        self.ffi = FFI()        
        self.ffi.cdef('''

        struct detection_struct{
            unsigned int x;
            unsigned int y;
            unsigned int w;
            unsigned int h;
            char label[256];
            double score;
            unsigned int class_id;
        };

        void blob_detector(unsigned int img_w, unsigned int img_h, unsigned char* img_t, double threshold, unsigned int *num_dets, struct detection_struct** dets);

        ''')

        # Load the blob detector library
        self.blob_det_lib = self.ffi.dlopen(lib_location)

        # instantiate the ffi data types that are going to be used for the C/C++ interface to the blob detector
        self.num_dets = self.ffi.new('unsigned int *')
        self.dets = self.ffi.new('struct detection_struct**')

        # threshold setting for the detection
        self.threshold = 30.0

        # Create category index
        # self.category_index = lm_util.create_category_index_from_labelmap(self.labels_path,
        #                                                                   use_display_name=True)

    def _get_detections_from_frame(self, frame):

        # convert image to grayscale if it is a multi-channel image
        if(len(frame.pixels.shape) >= 3):
            img_h, img_w, _ = frame.pixels.shape
            img = (frame.pixels[:, :, 0] * 299/1000) + (frame.pixels[:, :, 1] * 587/1000) + (frame.pixels[:, :, 2] * 114/1000)
        else:
            img_h, img_w = frame.pixels.shape
            img = frame.pixels

        # normalize image and convert to uint8
        img = np.uint8(255 * ((img - img.min()) / (img.max() - img.min())))

        # Perform detection
        # output_dict = self._detect_fn(input_tensor)
        self.blob_det_lib.blob_detector(img_w, img_h, img.tobytes(), self.threshold, self.num_dets, self.dets)

        # All outputs are batches tensors.
        # Convert to numpy arrays, and take index [0] to remove the batch dimension.
        # We're only interested in the first num_detections.
        num_detections = self.num_dets[0]
        # output_dict = {key: value[0, :num_detections].numpy()
        #                for key, value in output_dict.items()}

        # Extract classes, boxes and scores
        # classes = output_dict['detection_classes'].astype(np.int64)  # classes should be ints.
        # boxes = output_dict['detection_boxes']
        # scores = output_dict['detection_scores']

        # Form detections
        detections = set()

        for idx in range(num_detections):
            metadata = {
                "raw_box": np.array([dets[0][idx].x/img_w, dets[0][idx].y/img_h, (dets[0][idx].x+dets[0][idx].w)/img_w, (dets[0][idx].y+dets[0][idx].h)/img_h]).astype(np.float32),
                "class": dict(id=dets[0][idx].class_id, name=self.ffi.string(dets[0][idx].label).decode("utf-8")),
                "score": dets[0][idx].score
            }            

            state_vector = StateVector(np.array([[dets[0][idx].x],
                                        [dets[0][idx].y],
                                        [dets[0][idx].w],
                                        [dets[0][idx].h]]).astype(np.float64))

            detection = Detection(state_vector=state_vector,
                                  timestamp=frame.timestamp,
                                  metadata=metadata)                                        

            detections.add(detection)

        # for box, class_, score in zip(boxes, classes, scores):
        #     metadata = {
        #         "raw_box": box,
        #         "class": self.category_index[class_],
        #         "score": score
        #     }
        #     # Transform box to be in format (x, y, w, h)
        #     state_vector = StateVector([box[1]*frame_width,
        #                                 box[0]*frame_height,
        #                                 (box[3] - box[1])*frame_width,
        #                                 (box[2] - box[0])*frame_height])
        #     detection = Detection(state_vector=state_vector,
        #                           timestamp=frame.timestamp,
        #                           metadata=metadata)
        #     detections.add(detection)

        return detections
